---
layout: presentation
title: "Introduction to Decision tree"
permalink: /slide/dt
description: ""
category: slide
tags: [mr]
---

class: center, middle, inverse

# 决策树简介
章恒
.footnote[Power by Remark.js & MathJax.js]

???
上一次分享时，我们以线性回归这一个非常简单的算法为例子，给大家科普了模型、算法、损失函数以及机器学习里面的一些重要的概念，相信大家对模型这个黑盒子应该有了一个初步的认识，今天则是介绍另外一个传统、经典的模型，决策树。
开始之前，先简单回顾下线性回归，顺便加强下对机器学习的三要素的认识
---

.left-column[
  ## review
  ### - linear regression
]
.right-column[

模型：\\(\hat{y} = h_{\theta} (\textbf{x}) = \theta^{T} \cdot \textbf{x}\\)

策略：\\(J(\theta) = \textbf{MSE} = \frac{1}{m} \sum_{i=1}^{m} (\theta^{T}\cdot\textbf{x}^i - y^i)^2\\)

算法：normal equation、 Gradiend desent

<img src="/public/fig/mr_17.png" width="500" height="450">
]

???


---
.left-column[
  ## review
  ### - elements
]
.right-column[

### 模型
 - \\(Y = f(X)\\) （基于决策函数）
 - \\(Y = arg max_{c_k}P(c_k|X)\\) （基于概率分布）

### 策略

 - 0-1损失函数  \\(L(Y,f(X)) = (Y != f(X))\ ?\ 1 : 0\\)
 - 平方损失函数  \\(L(Y, f(X)) = (Y - f(x))^2 \\)
 - 绝对损失函数  \\(L(Y, f(X)) = abs((Y - f(x)) \\)
 - 对数损失函数  \\(L(Y, f(X)) = -log(P(Y|X)) \\)

### 算法
 - 最优化问题
 - 随着模型假设空间、策略的不同而不同

]

???
为什么我们需要机器学习方法呢？

对于垃圾邮件过滤这个问题，我们采用传统的基于规则、显式编程的方式也能解决，比如可能会有这样
一个闭环，我们先分析问题，然后编写一堆判断垃圾邮件的规则、评价是否有效，没问题的话上线
有问题的话分析误判的case，再对规则做修正

很明显，这样需要开发人员反复编写大量复杂的判断规则，难于维护，机器学习方法则是从数据中学习
出表征垃圾邮件的规则和因素，程序易维护、准确率高。这是一方面

---
class: center, middle, inverse

# 什么是决策树

???


---
.left-column[
  ## desicion tree
  ### - model
]
.right-column[
 ### 决策树模型

<img src="/public/fig/dt1.png" width="730" height="500">

]

???
另一方面，如果垃圾邮件的发布者们升级了，成功绕开了已有的判断规则，那么这时
传统的基于规则方法就很痛苦，需要根据新形式的垃圾编写新的规则，再测试验证
而机器学习方法只需拿新的数据重新跑下模型就可以，而获取数据、训练模型
这个过程通常都是容易自动化进行的，也就是说能够做到模型随着数据自我进化


---
.left-column[
  ## desicion tree
  ### - entropy

  <img src="/public/fig/dt2.png" width="300" height="260">
]
.right-column[
 ### 熵

 - 香农信息论里最基本的概念
 - 度量的是随机变量的不确定性

 - 假设X是一个离散型随机变量，有n个不同的取值，概率分布情况是：
 
 $$P(X = x_i) = p_i, i =1,2,...,n$$

 那么，X的熵为

 $$H(X) = -\sum_{i=1}^{n}p_ilogp_i$$

 - 当随机变量只有两个离散值时（比如0和1）\\(P(X=1) = p, P(X=0) = 1-p\\)

 $$H(p) = -p log_2p - (1-p)log_2(1-p)$$

 举例： 地球明天会不会毁灭、抛一枚硬币出现的正面还是反面


]
???

---
.left-column[
  ## desicion tree
  ### - loss
]
.right-column[
### 怎么利用熵来度量决策树的拟合训练数据集的好坏？

<img src="/public/fig/dt1.png" width="600" height="400">

$$Cost(T) = \sum_{t=1}^{\|T\|}N_tH_t(T)$$


]
???


---
.left-column[
  ## desicion tree
  ### - algorithm
]
.right-column[
### 怎么最小化决策树的代价函数

- 从所有决策树空间中搜索最优决策树是个NP-complete问题

- 存在很多启发式算法: ID3、C4.5、CART

- 基本思想：选择一个最优的特征，根据这个特征来对训练数据集进行分割，对分割产生的各个子数据集递归地重复这个过程，直至将各个子数据集基本分类。
这个选择特征，再分割训练数据集的过程就是决策树的生成过程。

- 剪枝：
  - 为了防止过拟合
  - 预剪枝和后剪枝

]
???
还是以识别西瓜好坏为例，刚开始先构建根节点，我们经过对数据集的统计发现纹理这个特征对瓜好坏的区分度是最高的，（最优特征）
，所以先采用纹理的不同来分割数据集，得到三个子数据集。对于这个子数据集，我们发现里面的样本基本都是坏瓜了，意味着我们不需要再
去对他找别的特征来进行分割，就形成了一个叶子节点。
对于其他数据集，比如第一个，其中有好瓜也有坏的，对这个数据集我们再统计发现根蒂的形状对区分好坏有很大帮助，因此再根据根底形状进行分割，
递归地重复这个过程，直至所有的子数据都已基本分类。那么一颗决策树就构建好了。

为什么要选择这样的启发式策略，从直观上来看，这样做倾向于生成一个深度浅的树，从而叶子节点数少，从而近似最优化这个代价函数

---
class: center, middle, inverse

# 怎么选择最优特征

???


---
.left-column[
  ## desicion tree
  ## algorithm
  ### - feature selection
]
.right-column[
### 信息增益

假设训练数据集D，\\(\|D\|\\)表示其样本容量，有K个分类: \\(C_1,C_2,C_3,...,C_k\\), \\(\|C_k\|\\)是第k类的样本个数

那么：\\(\sum_{k=1}^{K}\|C_k\|\\) = \|D\|

假设特征A有n个不同的取值{\\(a_1, a_2, ..., a_n\\)}，特征A的不同取值将D划分为n个不同子数据集\\(D_1,D_2,...,D_n\\)，
\\(\|D_i\|\\)是第i个子集中的样本个数。

记子集\\(D_i\\)中属于类 \\(C_k\\) 的样本集合为:

\\(D_{ik} = D_i \cap C_k\\)

- 定义：数据D的熵为H(D): \\(H(D) = -\sum_{k=1}^{K}\frac{\|C_k\|}{\|D\|}log\frac{\|C_k\|}{\|D\|}\\)

- 定义: 特征A对数据集D的条件熵H(D|A): \\(H(D|A) = -\sum_{k=1}^{K}\frac{\|D_i\|}{\|D\|}H(D_i) \\)

- 定义: 特征A的在数据D的信息增益g(D,A): \\(g(D,A) = H(D) - H(D|A)\\)

]
???
现在对于给的一个数据集，我们可以通过这个算法来计算各个特征的信息增益，选择信息增益最大的特征作为最优特征。
ID3算法就是这样干的

---
.left-column[
  ## desicion tree
  ## algorithm
  ### - ID3/C4.5
]
.right-column[
### ID3
- 使用信息增益作为最优特征的选择依据


### C4.5
- 使用信息增益比作为最优特征的选择依据
  $$g_r(D,A) = \frac{g(D,A)}{H(D)}$$

]
???

---
.left-column[
  ## desicion tree
  ## algorithm
  ### - pruning

  <img src="/public/fig/dt3.png" width="300" height="300">
]
.right-column[
### 剪枝
- 预剪枝：在生成决策树过程中就采取策略防止模型过复杂，比如限制树的最大深度、设置子数据集的熵阈值

- 后剪枝：在生成完决策树后对树进行修剪。

  $$C_α(T) = Cost(T)+α\|T\|$$

  算法思想

   - 计算树每个叶子结点的经验熵。
   - 递归地从叶节点向上回缩：设一叶结点回缩到父结点之前和之后，树分别是TB和TA，其对应的损失函数值分别是Cα(TB)与Cα(TA)，如果Cα(TA)≤Cα(TB)，则剪枝，即将父节点变成新的叶结点。

]
???


---
.left-column[
  ## desicion tree
  ## algorithm
  ### - CART

]
.right-column[
### CART（分类与回归树）

- 更为通用和广泛，可以用于分类也可以用于回归，分类树和回归树

- 二叉树，分支都是是和否（类似于把特征做onehot编码）

- 算法思想：选择特征和划分，递归生成决策树，剪枝

<img src="/public/fig/dt4.png" width="600" height="350">


]
???

---
.left-column[
  ## desicion tree
  ## algorithm
  ### - CART

]
.right-column[
### 分类树

- 定义：Gini指数 \\(Gini(D) = \sum_{k=1}^{K}p_k(1-p_k) \\)

  \\(= 1 - \sum_{k=1}^{K}p_k^2\\)
  
  \\(= 1 - \sum_{k=1}^{K}(\frac{\|C_k\|}{\|D\|})^2\\)

  Gini指数度量的是数据集D的不确定性（和熵类似）

- 将数据集D按照特征A是否取某一个值a来划分，可分割成两部分，\\(D_1\\)和\\(D_2\\), 
  
  定义条件基尼指数：\\(Gini(D,A) = \frac{\|D_1\|}{\|D\|}Gini(D_1) + \frac{\|D_2\|}{\|D\|}Gini(D_2)\\)

  度量数据集D在经过A=a 分割后的数据集D的不确定性 （和条件熵类似）

]
???


---
.left-column[
  ## desicion tree
  ## algorithm
  ### - CART

]
.right-column[
### 分类树生成算法

输入： 训练数据集D，停止计算条件

输出： CART分类树

根据训练数据集，从根节点开始，递归地对每个节点进行以下操作，构建二叉决策树

1. 对现有特征A的每一个特征，每一个可能的取值a，都将数据集D分割成D1和D2两部分，计算A=a时的基尼指数。

2. 在所有可能的特征以及它们所有可能得切分点中，选择条件基尼指数最小的特征及其对应的切分点作为最优特征和最优切分点。根据这个特征和切分点生成两个子节点，并将训练数据集
分配到这两个子节点中去。

3. 递归调用，直到满足停止条件。

]
???
这里的停止条件一般有这么几种：节点中的样本个数小于预定的阈值，样本集的基尼指数小于预定的阈值，或者没有更多可供选择、划分的特征了


---
.left-column[
  ## desicion tree
  ## algorithm
  ### - CART

]
.right-column[
### 回归树

- 回归树适用于那些输入和输出都是连续变量的回归问题

- 生成一颗树模型，将输入空间划分成M个子单元\\(R_1,R_2,..., R_m\\)，每个子单元上有个固定的输出值\\(c_m\\)
  因此，回归树的模型可以用数学语言表示成：

  $$ f(x) = \sum_{m=1}^{M}c_mI(x \in R_m) $$


.left[
<img src="/public/fig/dt5.png" width="350" height="300"> 
]

.right[
<img src="/public/fig/dt6.png" width="400" height="300">
]

]
???
所以回归树模型的生成面临两个问题：1）怎么划分特征空间 2）划分好后怎么决定每个子单元上的输出值

---
.left-column[
  ## desicion tree
  ## algorithm
  ### - CART

]
.right-column[
### 怎么决定每个子单元上的输出值
  - 假设特征空间的划分确定，使用平方误差来表示这颗回归树对训练数据的预测误差：
    $$ Cost(T) = \sum_{x_i \in R_m} (y_i - f(x_i))^2 $$

    最小化Cost(T) 得到： \\( c_m = ave(y_i | x_i \in R_m) \\)

### 怎么来划分特征空间
  - 启发式方法：每次选择使得平方误差最小的划分方式，递归进行

  - 假设选择第j个特征x^(j)和它的取值s，作为切分特征和切分点，那么可以将原来的特征空间划分成两个子区域：

    $$ R_1(j,s) = \\{x|x^{j} <= s\\},  R_2(j,s) = \\{x|x^{j} > s\\}$$

    在\\(R_1\\)上输出为\\(c_1\\)，\\(R_2\\)上输出为\\(c_2\\)

]
???


---
.left-column[
  ## desicion tree
  ## algorithm
  ### - CART

]
.right-column[
### 怎么来划分特征空间

  - 用平方误差来刻画这个划分的代价：

    $$ min_{j,s} [min\ Cost(R_1) + min\ Cost(R_2)] $$

  - 对于每个特征j，可以通过最小化平方误差可以求出最优的切分点，构成一个(j,s)对，遍历所有的特征
    得到当前特征空间的最优的切分变量和最优切分点，选择这个划分将当前特征空间一分为二

  - 对每个子特征空间递归上述过程，直到满足停止条件，得到一颗最小二乘回归树

]
???


---
class: center, middle, inverse

# 总结

???
今天由于时间关系，就不给大家演示具体的实战部分了，下次分享再结合一个具体的例子来实操下
简单总结下今天的内容。
首先，我们大致介绍了下机器学习系统的定义和分类，以及都解决什么问题，有哪些算法和模型

然后，以线性回归为切入点，进入模型黑盒内部，介绍了线性回归模型表达、代价函数、normal equation
、各种梯度下降最优化算法、多项式回归、欠拟合过拟合、模型正则化等内容

最后，给出一个实际工程应用中使用机器学习解决问题的大致步骤

希望大家有所收获、谢谢

献给DW.