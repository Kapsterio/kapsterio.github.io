---
layout: post
title: "基于netty的抵用券发券服务化"
description: ""
category: 
tags: []
---
## 背景与现状
### 传统MVC模式的局限性
抵用券系统的管理配置后台和对外提供的接口都是基于php实现的，典型的mvc结构，对于mis后台而言，这种方式无可厚非，优点主要有：开发效率很高、方便业务迭代、代码结构清晰、维护简单。对于对外提供的接口而言，当请求并发量大的时候问题就来了，每一个打过来的请求都在FPM的一个进程中完成所有业务逻辑的处理，处理完了所有申请的资源又都释放了（包括数据库连接、KV实例等等），典型的资源浪费啊。基于java的MVC架构可能要比这个好点，由线程池里的线程来处理请求，同时可以维护一个数据库连接池、一个KV实例，每个请求来时如果要写数据库的话向数据库连接池去申请一个连接就可以，做到连接的复用。然而，无论是php还是JAVA，这种系统的架构都是Request Per Thread或者Request Per Process， 即每个请求都需要由一个独立的thread或者process来处理，各个请求处理相互独立。这样就有个问题，单个机器能撑住的并发量是非常有限的。

### 目前抵用券发券业务逻辑和实现
以抵用券发券为例，下图是原来处理每一个发券请求的业务逻辑。首先会查数据库，取出相应的抵用券活动，进行一些业务逻辑判断；再和tair进行交互，做些防止超发的校验；最后然后再和数据库交互，这里向数据库中写入一条抵用券记录。
//插图

逻辑比较简单，但是对于PHP而言，每个请求都会先和数据库建立连接、会和tair建立连接，和数据库、tair进行读写交互，最后释放数据库和tair的连接。上述逻辑实现起来也非常简单，单台机器的QPS跟不上也没事，因为可以很方便的水平扩展，一台300的话，十台机器就能抗住3K的量，但问题在于促销数据库得撑住光这一个发券接口就有每秒3K的写入量。在接口流量高峰期时，数据库的写入压力还是蛮大的，继而可能还会导致一些主从不一致的问题。

为了缓和这个问题，目前给出一个解决方案是——提供一个异步发券接口，即在一个发券请求处理过程中并不执行实际的数据库写入操作，而是向消息队列（rabbitmq）中添加一个消息后接口就返回了，告诉调用方能否发券。由消息队列的消费者去执行实际的写数据库操作，此时我们可以通过一次性取出多个消息，批量地写数据库，来减少了数据库的写入次数。但是这个解决方案有两个问题：1）虽然从接口返回到抵用券实际被写入数据库这一时间间隔一般来说很短，但不可控，可能出现接口返回給调用方说可以发券
，但是消息队列消费者在实际执行时，发现活动结束或者其他原因导致发券失败。2）存在一些业务方在调完发券接口后立即就需要使用。因此，这个解决方案显然是存在缺陷的。

## 基于生产者-消费者模型的解决方案
基于前面的分析，整个发券接口的瓶颈在于数据库的写入这块。为了保证接口返回成功发券时数据已经写入数据库，同时提升服务的承载能力，必须摒弃之前的这种Request Per Process方式。

一个合理的想法是由单独的API线程去处理socket请求，并且不直接为每次发券请求都去写一次数据库，而是将生成的券信息和相应的socket存入一个内存中的queue中，由另外的线程异步地从queue中批量取出券信息，组合成一条写数据库sql，再执行写操作，然后响应所有相对应的socket。
这是一个典型的生产者-消费者模型，生产者是API线程，负责往queue里写任务，消费者是数据库写入线程，负责从queue里批量取任务。

这样做有两个好处：
- 1）接口返回给调用方时，券已经写入到数据库中了，不存在之前异步发券有的问题
- 2) 性能会有很大提高，数据库写入压力也会减少


这里给出一个基于生产者-消费者模型的发券服务的系统结构图：
//插图

图是关于发券服务主要工作线程和组件的示意图，主要有四种线程，两个主要的全局数据结构。

1. main线程，它主要负责： 首先从数据库中将所有在线的抵用券活动load进内存，存在一个全局的map中，key是活动的id,value是活动对象详细信息。一般来说，在线的抵用券活动最多几千个。第二，初始化并启动服务，（具体包括，装配一些实例、配置netty一些参数，注册一些业务handler，启动所有生产者、消费者、工作线程等等）。然后一直阻塞直到服务被shutdown，之后做一些后续的清理工作。

2. Eventloop线程，它主要负责接收和处理发券接口的socket请求，基于netty实现，netty用channel对象封装socket，可以理解为一个channel就对应着一个socket连接。channel上的每个I/O事件都会经过一个ChannelPipeline，根据事件的类型被pipeline中相应的handlers处理一下，这些handlers里的相应的方法就是我们实现具体业务逻辑的地方。原则上， handler里面的方法不能有阻塞、执行时间不可控的操作（比如数据库读写、同步调某个接口等），因为这样会拖慢在同一个eventloop线程上处理的所有事件。 在vas里，业务handler做的工作是先从内存的map里取出要发的券活动信息，做一些业务逻辑校验，所有校验通过后向，内存的queue里插入一个Task对象（Task对象里封装了券对象和channel）

3. DBWriter线程，也就是vas的消费者线程，它负责从queue中取出多个Task对象，然后构建一个数据库写入sql，执行后，响应这些task对象对应的channel

4. CampaignMaintainer线程，它是维护内存中的活动map的线程，做的工作就是定时去数据库中增量load抵用券活动的更新信息，然后update内存中的活动map

5. Campaign Map，装着所有在线的抵用券活动。首先它是一个hashmap，key是活动ID，value是活动对象的详细信息。由于这个数据结构会被多个线程并发读写，在实现时我采用jdk自带的CocurrentHashMap来作为活动的容器。

6. Task queue，任何生产者-消费者模型实现的时候都需要一个queue。这里，这个Task queue装着发券的Task对象，Task对象封装了券对象(CampaignCard)和Channel对象。由Eventloop线程往里写入，由DBWriter线程从里取出任务。同样，queue也会被多个线程并发读写，实现时采用了JDK自带的ArrayBlockingQueue。


## 实现过程中的几个具体问题

### 消费者从queue中取任务策略
前面提到，为了提升性能，消费者线程（DBWriter）在从Task queue中取任务时会批量取出多个任务，然后构建一个数据库写入sql，再执行。这一思路底层可以用条件变量（condition variable）来实现，即初始化一个条件变量，生产者在写入任务时判断queue中任务数是否大于某个阈值，是的话唤醒等待在这个条件变量上的消费者线程(cond_singal)。而消费者线程在循环中先阻塞在这个条件变量上（cond_wait），等待被唤醒，被唤醒后再去queue中取任务。

那么问题来了，应该批量取出多少任务，怎么取？如果固定地取batchNum个任务（假设batckNum>1）,存在一个问题：当服务在某个时间内请求量较少，queue中攒满batchNum个任务可能还需要很长时间，导致这一批任务对应的所有发券请求的响应时间也就变得很长，甚至超时，这显然不合适。

目前我的做法是，基于JDK的ArrayBlockingQueue实现，ArrayBlockingQueue的行为类似一个有条件变量的Queue。当queue为空时，从queue中取任务的操作(take)将被阻塞，直到queue非空；当queue满时，往queue里塞任务的操作（offer）将被阻塞，直到queue非满；当queue非空也非满时，take和offer操作都不会阻塞。

基于性能和响应时间的两方面考虑，在实现中开启8个消费者线程，每个线程先以阻塞的方式地从任务队列中取出一个任务(take)，然后以非阻塞方式取出多个任务（poll），最多取出batchNum个任务，目前batchNum值为30。这样保证了：

- 1）在没有任务时消费者线程不会去轮询cpu
- 2）系统请求量少的时（攒满batchNum个任务需要一段时间）也会有很快的响应速度。

### shutdown and cancellation
> Dealing well with failure, shutdown, and cancellation is one of the characteristics that distinguish a well behaved application from one that  merely works                ----<Java Concurrency In Pratice>
vas里有多个长期运行的线程，比如netty的EventLoop线程、消费者DBWriter线程、活动维护线程、服务运行状态监控线程等。以DBWriter线程为例，它会从内存中的任务队列中取任务，当需要把服务shutdown时，如果不对DBWriter等线程额外的shutdown处理，线程将随着JVM进程一起直接退出，可能导致任务队列里还残留一些任务没有被处理，而一个well behaved的应用程序应该提供一个完善的shutdown机制。

在vas中，DBWriter提供了一个shutdown方法，用于向正在工作中的DBWriter发起shutDown请求，DBWriter收到shutDown请求后不立即退出，而是等到任务队列为空时，才真正结束工作。

netty本身提供了对EventLoop的gracefully shutdown的支持，调用EventLoopGroup的shutdownGracefully方法即可，一旦调用，netty就不在接受新的连接请求，在处理完已建立的连接请求后EventLoop才结束。

最后，什么时候去调用这写shutdown方法? Java并没有提供Unix信号相关的API，使得我们没法去直接捕获信号，所幸Java提供了shutdownhook机制，可以为JVM进程注册一个shutdownhook，当JVM进程收到Term信号时，这个shutdownhook就会被执行，代码如下：

{% highlight java linenos %}
 Runtime.getRuntime().addShutdownHook(new Thread() {
     @Override
     public void run() {
         System.out.println("shuting down...");
         bossGroup.shutdownGracefully();
         workerGroup.shutdownGracefully();
         for (int i = 0; i < writerNum; i++) {
            dw[i].shutDown();
         }
         tair.getTairClient().close();
         cm.interrupt();
         monitor.interrupt();
     }
 });
{% endhighlight %}

### Http服务以及路径转发
vas目前提供的是发券的http服务，后续可能扩展提供更多的发券相关服务，则会根据不同的请求path区分所请求的服务。为了增加vas的可扩展性，我实现了一套基于netty的路径转发机制。

前面提到，netty的channel上的所有I/O事件都会经过一个ChannelPipeline，ChannelPipeline中是我们的各个业务handlers。netty的强灵活性就体现在pipeline中的这些业务handlers可以动态的增加或删除。基于这个特性，我实现了一个Controller handler，所有的请求（Input事件）都会被这个Controller处理，目前Controller里面做两件事：
- 1）接口的访问权限检查（BA认证）
- 2）路径转发，即根据http请求的uri中的path，去一个被依赖注入的map中，找这个path对应的业务handlers，然后动态更新pipeline，加载这些对应的业务handlers。部分代码如下：

{% highlight java linenos %}
QueryStringDecoder queryStringDecoder = new QueryStringDecoder(request.uri());
String path = queryStringDecoder.path();
HttpHeaders headers = request.headers();
boolean keepAlive = HttpUtil.isKeepAlive(request);
//根据path和headers来处理BA认证逻辑
if (checkBA(path,headers)) {
    ArrayList<ChannelHandler> handlers = handlersOfPath.get(path);
    if (handlers == null) {
        //系统不提供这种服务
        HttpResponseUtil.writeResponse(channelHandlerContext, "service not found", keepAlive);
        return;
    }
    for (ChannelHandler handler : handlers) {
        channelHandlerContext.pipeline().addLast(handler);
    }
    ReferenceCountUtil.retain(request);
    channelHandlerContext.fireChannelRead(request);
} else {
    HttpResponseUtil.writeResponse(channelHandlerContext, "auth failed", keepAlive);
}
{% endhighlight %}

此外，考虑到一次请求处理完了后，底层的TCP连接可能还保持，这个连接上的下一次请求的path可能不一样了，因此在每次请求处理完以后，需要将之前controller里面动态加载的handler再remove掉。这个逻辑通过给pipeline中增加一个自定义的CleanHandler来实现，所有的output事件都会被CleanHandler处理，它只做一件事，就是remove掉那些在pipeline中位于它之后的业务handers。部分代码如下：

{% highlight java linenos %}
while(ctx.pipeline().last() != this) {
     ctx.pipeline().removeLast();
}
super.write(ctx, msg, promise);
{% endhighlight %}

综上，vas里自定义的handlers分为两类，业务handlers(具体处理发券逻辑的handler)和业务无关handlers（像是ControllerHandler、CleanHandler），他们都是无状态的，可以每类handler可以只有一个对象（单例），为所有channel所共享。vas里关于http协议解析部分使用了netty自带的一些handler（HttpRequestDecoder、HttpResponseEncoder、HttpObjectAggregator），这些handler是有状态的---解析http协议所需要的状态机。下面是vas关于netty的配置和初始pipeline的组成:

{% highlight java linenos %}
ServerBootstrap b = new ServerBootstrap();
b.group(bossGroup, workerGroup)
        .channel(NioServerSocketChannel.class)
        .option(ChannelOption.SO_BACKLOG, 8192)
        .handler(new MyLogHandler(qpsCounter, LogLevel.DEBUG))
        //.childHandler(new VasServerInitializer(map, tair, queue, producingCounter));
        .childHandler(new ChannelInitializer<SocketChannel>() {
            @Override
            protected void initChannel(SocketChannel socketChannel) throws Exception {
                ChannelPipeline p = socketChannel.pipeline();
                p.addLast(new HttpRequestDecoder());
                p.addLast(new HttpResponseEncoder());
                p.addLast(new HttpObjectAggregator(1048576));
                p.addLast(controller);
                p.addLast(cleaner);
            }
        });
{% endhighlight %}

## 技术选型
### netty
netty是一款优秀的异步事件驱动网络编程框架，其对java自带的NIO有着很好的封装，提供了很友好的编程API，同时它通过Zero-copy、堆外内存的ByteBuffer等技术获得了很好性能。它在国内外各大互联网都有很广泛的应用。选择netty的另外一个原因是它还提供了对各种协议的解析工具（http, https, websocket, protobuf等等），省了不少事。与其同类型的开源产品还有apache 的mina，也是一个作者写的，不过貌似已经不维护了。

vas里面使用的是最新4.1.x的稳定版，netty4和netty3API差别还是蛮大的，并且不兼容。

### mybatis
为了省事，使用mybatis这一ORM框架来操作数据库，数据源直接用的JDBC的数据库连接池。

### slf4j和log4j1.2
日志工具用的是slf4j组合log4j1.2，看公司其他组都是大部分都是这么干的，虽然看log4j官网上说建议使用log4j 2.x，2.x和1.2的配置、API也都不一样

### fastjson
接口以json格式输出内容，序列化工具采用阿里的fastjson，不得不说使用起来很是方便

### 用到java里面的一些奇技淫巧
- 反射：vas里面有个地方需要把一个model对象（CampaignCard）的属性和值取出来，存到一个hashmap中，key为属性名，value是属性值，为了追求优（lan）雅（duo），使用了java提供的反射机制，代码如下：
{% highlight java linenos %}
HashMap<String, Object> response = new HashMap<String, Object>();
CampaignCard card = (CampaignCard)msg;
for (Field field : CampaignCard.class.getFields()) {
    response.put(field.getName().toLowerCase(), field.get(card));
}
{% endhighlight %}

- 泛型：Campaignbase表有个logic字段，内容是一个json格式的键值对，其值可能有好几种类型，在定义从logic里取值的getLogic方法时使用了泛型，代码如下：
{% highlight c linenos %}
public <T> T getLogic(String logicName, T defaultValue) {
        if (this.logicCache == null) {
            this.logicCache = JSONObject.parseObject(this.logic);
        }
        Object ret = this.logicCache.get(logicName);
        if (ret == null) {
            return defaultValue;
        } else {
            return (T)ret;
        }
}
{% endhighlight %}

## 编译的部署运行

## 待完善的地方